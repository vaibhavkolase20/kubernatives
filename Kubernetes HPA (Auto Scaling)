ЁЯУШ Kubernetes HPA (Auto Scaling) тАУ Complete Practical Documentation (Marathi)
ЁЯОп Objective

Traffic рд╡рд╛рдврд▓рд╛ рдХреА nginx pods automatically scale-up рд╣реЛрддреАрд▓
Traffic рдХрдореА рдЭрд╛рд▓рд╛ рдХреА pods auto scale-down рд╣реЛрддреАрд▓
рд╣реЗ рд╕рдЧрд│рдВ HPA (Horizontal Pod Autoscaler) рд╡рд╛рдкрд░реВрди.

ЁЯз▒ Environment Details (рддреБрдЭрдВ actual setup)

Kubernetes Version: v1.34.3

Nodes:

controlplane

node01

Container Runtime: containerd

OS: Ubuntu 24.04

CNI: Calico

Namespace: default

ЁЯФ╣ STEP 1: NGINX Deployment рддрдпрд╛рд░ рдХрд░рдгреЗ
Command
kubectl create deployment nginx-deployment --image=nginx

Verify
kubectl get deployment
kubectl get pods -o wide

Output (Expected)
nginx-deployment   1/1   Running


тЬЕ NGINX pod successfully running

ЁЯФ╣ STEP 2: NGINX рд╕рд╛рдареА Service рддрдпрд╛рд░ рдХрд░рдгреЗ
ClusterIP Service
kubectl expose deployment nginx-deployment \
--name=nginx-service \
--port=80 \
--target-port=80 \
--type=ClusterIP

Verify
kubectl get svc

ЁЯФ╣ STEP 3: Metrics Server Install рдХрд░рдгреЗ (HPA рд╕рд╛рдареА MUST)
Command
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

Verify
kubectl get pods -n kube-system | grep metrics


тЭМ Problem (Live Error):

metrics-server   0/1   Running

ЁЯФз STEP 3.1: Metrics Server Error Fix (Important)
Reason

Kubelet TLS issue тЖТ metrics collect рд╣реЛрдд рдирд╡реНрд╣рддреЗ

Solution
kubectl edit deployment metrics-server -n kube-system

args рдордзреНрдпреЗ add рдХреЗрд▓реЗ:
- --kubelet-insecure-tls

Final args section
- --cert-dir=/tmp
- --secure-port=10250
- --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
- --kubelet-use-node-status-port
- --metric-resolution=15s
- --kubelet-insecure-tls

Verify
kubectl get pods -n kube-system | grep metrics


тЬЕ 1/1 Running

ЁЯФ╣ STEP 4: Load Generator Pod рддрдпрд╛рд░ рдХрд░рдгреЗ (Traffic рд╕рд╛рдареА)
kubectl run load-generator --image=busybox --restart=Never -- /bin/sh -c "sleep 3600"


Verify:

kubectl get pods

ЁЯФ╣ STEP 5: HPA Create рдХрд░рдгреЗ
kubectl autoscale deployment nginx-deployment \
--cpu-percent=50 \
--min=1 \
--max=5

Verify
kubectl get hpa


Output:

cpu: 0%/50%
replicas: 1

ЁЯФ┤ LIVE ISSUE: Pods scale рд╣реЛрдд рдирд╡реНрд╣рддреЗ
Observation
CPU: 33тАУ40%
TARGET: 50%
REPLICAS: 2

тЭМ Root Cause

NGINX deployment рдордзреНрдпреЗ CPU request / limit defined рдирд╡реНрд╣рддреЗ

ЁЯСЙ HPA formula:

CPU Utilization = Used CPU / Requested CPU

ЁЯФз STEP 6: Deployment рдордзреНрдпреЗ CPU Request & Limit Add рдХрд░рдгреЗ
Command
kubectl edit deployment nginx-deployment

Add this inside container section
resources:
  requests:
    cpu: "100m"
  limits:
    cpu: "200m"

Full Example
containers:
- name: nginx
  image: nginx
  ports:
  - containerPort: 80
  resources:
    requests:
      cpu: "100m"
    limits:
      cpu: "200m"


ЁЯУМ Save & Exit

ЁЯФ╣ STEP 7: Traffic Generate рдХрд░рдгреЗ (Scale UP рдкрд╛рд╣рдгреНрдпрд╛рд╕рд╛рдареА)
load-generator рдордзреНрдпреЗ login
kubectl exec -it load-generator -- /bin/sh

Inside pod
while true; do wget -q -O- http://nginx-service; done


(2тАУ3 terminals рдордзреНрдпреЗ рд╣реЗ command рдЪрд╛рд▓рд╡рд▓рдВ рддрд░ CPU рдЬрд╛рд╕реНрдд рд╡рд╛рдврддреЛ)

ЁЯФ╣ STEP 8: HPA Live Observe рдХрд░рдгреЗ
kubectl get hpa -w

Expected Output
cpu: 55%/50%   replicas: 3
cpu: 70%/50%   replicas: 4
cpu: 85%/50%   replicas: 5

Verify Pods
kubectl get pods


тЬЕ Multiple nginx pods Running

ЁЯФ╣ STEP 9: Auto Scale DOWN Test
Traffic рдерд╛рдВрдмрд╡
CTRL + C
exit

Observe
kubectl get hpa -w


тП│ 2тАУ5 minutes рдирдВрддрд░:

replicas: 5 тЖТ 4 тЖТ 3 тЖТ 2 тЖТ 1

this is the link of this task
https://chatgpt.com/share/694e5026-8de4-800f-b3c2-b22e579459cd
